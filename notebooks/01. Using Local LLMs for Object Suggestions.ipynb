{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using Local LLMs for Object Suggestions**\n",
    "One of my first forays into building tools on top of Capacities will be \"object suggestions\". Basically: when prompted with all of my Object types + a particular Object (say, a Daily Note), then I'll use a locally hosted LLM (served via `ollama`) to parse out possible suggestions for objects to create. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension, which will automatically load in new code as it's written\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll import some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "from IPython.display import display, Markdown\n",
    "from pydantic import BaseModel, Field\n",
    "from ollama import chat\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Project imports\n",
    "from utils.data import parse_capacities_export_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring `ollama`\n",
    "What local model should I use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the model for Ollama\n",
    "# Ones I've downloaded include:\n",
    "# [\"qwen3:4b\", \"gemma3:4b\", \"deepseek-r1:1.5b\", \"deepseek-r1:7b\", \"gemma3:12b\"]\n",
    "ollama_model = \"gemma3:12b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "First, I'm going to load in my Capacities export data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path to the Capacities export .zip file\n",
    "capacities_export_zip_path = \"D:/data/datasets/capacities-export-data/Daily Export (fd84f574)/Everything (2025-06-02 21-45-56).zip\"\n",
    "\n",
    "# Load the capacities data from the specified .zip file\n",
    "capacities_data_df = parse_capacities_export_zip(capacities_export_zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Prompt\n",
    "Next up: I'll define the prompt that I'll use to identify objects within the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_types_of_interest = [\n",
    "    \"VideoGame\",\n",
    "    \"TvShow\",\n",
    "    \"Technology\",\n",
    "    \"StageShow\",\n",
    "    \"Person\",\n",
    "    \"Event\",\n",
    "    \"Book\",\n",
    "    \"BoardGame\",\n",
    "]\n",
    "# object_types_of_interest = [\n",
    "#     object_type\n",
    "#     for object_type in capacities_data_df[\"object_type\"].unique()\n",
    "#     if object_type not in [\"DailyNote\"]\n",
    "# ]\n",
    "\n",
    "prompt = f\"\"\"# Role\n",
    "I'd like you to act as a \"Named Entity Recognition\" (NER) system of sorts. \n",
    "\n",
    "# Context\n",
    "Users will provide you with a note from an export of their personal knowledge management system. \n",
    "\n",
    "\"Linked entities\" will be in Markdown format, like this:\n",
    "```\n",
    "[some text](EntityType/entity-id.md)\n",
    "```\n",
    "\n",
    "# Task\n",
    "\n",
    "Your task is to identify \"unlinked\" entities in the note. \n",
    "\n",
    "These \"unlinked\" entities will be entities that ought to be a new PKM Object, but aren't currently \"linked\" to anything. \n",
    "\n",
    "As follows are the possible entity types you should be looking for: \n",
    "\n",
    "{\"\\n\".join([\"- \" + entity_type for entity_type in object_types_of_interest])}\n",
    "\n",
    "# Suggestions\n",
    "\n",
    "- You can ignore any objects that are already linked in this way, as the user has already created a PKM Object for them.\n",
    "- If you find an entity that doesn't match any of the object types above, you can ignore it.\n",
    "- In your `reasoning` field, make sure to leverage your encyclopedic knowledge of the world's knowledge to add any necessary context to the entity\n",
    "\n",
    "# Response Format\n",
    "You'll respond with a JSON list of `PotentialObject` dictionaries. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PotentialObject(BaseModel):\n",
    "    \"\"\"An object that could potentially be created in the PKM system\"\"\"\n",
    "\n",
    "    text_snippet: str = Field(\n",
    "        ...,\n",
    "        description=\"A short, exact-text snippet (around ~10 words) from the note that contains the potential object\",\n",
    "    )\n",
    "\n",
    "    new_object_title: str = Field(..., description=\"The title of the potential object\")\n",
    "\n",
    "    new_object_type: str = Field(..., description=\"The type of the potential object\")\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"Roughly 1-2 sentences explaining why this is a potential object and why it should be created in the PKM system\",\n",
    "    )\n",
    "\n",
    "    make_new_object: bool = Field(\n",
    "        ...,\n",
    "        description=\"Based on the `reasoning`, whether the user should create a new object for this potential object\",\n",
    "    )\n",
    "\n",
    "\n",
    "class PotentialObjectsResponse(BaseModel):\n",
    "    \"\"\"A response containing a list of potential objects\"\"\"\n",
    "\n",
    "    potential_objects: list[PotentialObject] = Field(\n",
    "        ..., description=\"A list of potential objects that could be created\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Completions\n",
    "Next up: we can use the aforementioned prompt to process my Capacities data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize the completion generation\n",
    "max_chars_per_text = 7_500\n",
    "\n",
    "# Filter the capacities_data_df to only show DailyNote objects\n",
    "daily_notes_df = capacities_data_df.query(\"object_type == 'DailyNote'\").copy()\n",
    "\n",
    "# Iterate through each of the DailyNotes in the capacities_data_df and collect the potential objects\n",
    "df_idx_to_potential_objects_map = {}\n",
    "for idx, row in tqdm(\n",
    "    iterable=list(daily_notes_df.iterrows()),\n",
    "    desc=\"Generating potential objects for DailyNotes\",\n",
    "    total=len(daily_notes_df),\n",
    "    unit=\"note\",\n",
    "):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Extrtact the text content of the note\n",
    "        text_content = row[\"text_content\"]\n",
    "\n",
    "        # Generate suggestions using ollama\n",
    "        response = chat(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": text_content[:max_chars_per_text]},\n",
    "            ],\n",
    "            model=ollama_model,\n",
    "            format=PotentialObjectsResponse.model_json_schema(),\n",
    "            options={\"num_ctx\": 4096},\n",
    "        )\n",
    "\n",
    "        # Parse the response into a PotentialObjectsResponse object\n",
    "        potential_object_list = PotentialObjectsResponse.model_validate_json(\n",
    "            response.message.content\n",
    "        )\n",
    "\n",
    "        # Append the potential objects to the list\n",
    "        df_idx_to_potential_objects_map[idx] = potential_object_list.potential_objects\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_to_modeldumped_potential_objects_map = {\n",
    "    idx: [potential_object.model_dump() for potential_object in potential_objects]\n",
    "    for idx, potential_objects in df_idx_to_potential_objects_map.items()\n",
    "}\n",
    "\n",
    "potential_objects_df = daily_notes_df.copy()\n",
    "\n",
    "# Add the potential objects to the DataFrame\n",
    "potential_objects_df[\"potential_objects\"] = potential_objects_df.index.map(\n",
    "    df_idx_to_modeldumped_potential_objects_map\n",
    ")\n",
    "\n",
    "# Save the potential_objects_df to a JSON file\n",
    "potential_objects_df.to_json(\n",
    "    \"data/potential_objects.json\",\n",
    "    orient=\"records\",\n",
    "    force_ascii=False,\n",
    "    indent=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_potential_objects_df = potential_objects_df.explode(\n",
    "    \"potential_objects\", ignore_index=True\n",
    ").copy()\n",
    "\n",
    "# Use pd.json_normalize to normalize the potential_objects column\n",
    "normalized_potential_objects_df = pd.json_normalize(\n",
    "    exploded_potential_objects_df[\"potential_objects\"]\n",
    ")\n",
    "\n",
    "normalized_potential_objects_df.groupby([\"new_object_type\", \"new_object_title\"]).size().sort_values(\n",
    "    ascending=False\n",
    ").head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
